<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>PROJECT BOSTON</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="resume project_files/libs/clipboard/clipboard.min.js"></script>
<script src="resume project_files/libs/quarto-html/quarto.js"></script>
<script src="resume project_files/libs/quarto-html/popper.min.js"></script>
<script src="resume project_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="resume project_files/libs/quarto-html/anchor.min.js"></script>
<link href="resume project_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="resume project_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="resume project_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="resume project_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="resume project_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">PROJECT BOSTON</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="boston-housing-prices-ml-analysis" class="level1">
<h1><strong>Boston Housing Prices ML Analysis</strong></h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This project examines the key factors influencing <strong>Boston home prices</strong> using <strong>machine learning models (KNN algorithm, step-wise regression, random forest)</strong>. The goal is to understand which factors drive <strong>median home values (<code>medv</code>)</strong> and to identify the <strong>best predictive model</strong>. We have 13 predictor variables to work with which can result in a large number of possible models. In this project, we will use a model with all 13 predictors (model 1), a model derived by analyzing correlations and random forest (model 2), and a model selected using step-wise regression (model 3). Then we’ll measure the predictive accuracy of these models by creating training and test data sets, using the training data to predict the test data, and analyze the performance results.</p>
<dl>
<dt><code>crim  🚨</code></dt>
<dd>
<p>per capita crime rate by town.</p>
</dd>
<dt><code>zn  🏡</code></dt>
<dd>
<p>proportion of residential land zoned for lots over 25,000 sq.ft.</p>
</dd>
<dt><code>indus  🏭</code></dt>
<dd>
<p>proportion of non-retail business acres per town.</p>
</dd>
<dt><code>chas 🌊</code></dt>
<dd>
<p>Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).</p>
</dd>
<dt><code>nox  🌫️</code></dt>
<dd>
<p>nitrogen oxides concentration (parts per 10 million).</p>
</dd>
<dt><code>rm  🏠</code></dt>
<dd>
<p>average number of rooms per dwelling.</p>
</dd>
<dt><code>age 🏚️</code></dt>
<dd>
<p>proportion of old owner-occupied units built prior to 1940.</p>
</dd>
<dt><code>dis 🚉</code></dt>
<dd>
<p>weighted mean of distances to five Boston employment centres.</p>
</dd>
<dt><code>rad 🛣️</code></dt>
<dd>
<p>index of accessibility to radial highways.</p>
</dd>
<dt><code>tax 💰</code></dt>
<dd>
<p>full-value property-tax rate per $10,000.</p>
</dd>
<dt><code>ptratio 📚</code></dt>
<dd>
<p>pupil-teacher ratio by town.</p>
</dd>
<dt><code>black ⚖️</code></dt>
<dd>
<p>1000(��−0.63)21000(Bk−0.63)2 where ��Bk is the proportion of blacks by town.</p>
</dd>
<dt><code>lstat 📉</code></dt>
<dd>
<p>lower status of the population (percent).</p>
</dd>
</dl>
</section>
<section id="data-inspection" class="level2">
<h2 class="anchored" data-anchor-id="data-inspection">Data Inspection</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>      crim         zn      indus       chas        nox         rm        age 
-0.3883046  0.3604453 -0.4837252  0.1752602 -0.4273208  0.6953599 -0.3769546 
       dis        rad        tax    ptratio      black      lstat       medv 
 0.2499287 -0.3816262 -0.4685359 -0.5077867  0.3334608 -0.7376627  1.0000000 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 22.53281</code></pre>
</div>
</div>
<p>The associations between predictor factors and medv were derived using a correlation matrix:<br>
<br>
From the results above, higher poverty rates are linked to lower property values, as seen by the highest negative connection (<strong>-0.7377</strong>) between <strong>medv</strong> and <strong>lstat</strong> (% of lower-status individuals).<br>
The highest positive correlation between <strong>medv</strong> and <strong>rm</strong> (number of rooms per house) is <strong>0.6954</strong>, indicating that larger homes are typically more costly. Higher pollution and poorer education have a negative effect on home prices, according to other variables that also have moderately negative correlations with <strong>medv</strong>, including <strong>nox</strong> (pollution). The <strong>mean</strong> (average) home price (<strong>medv</strong>) in the dataset is <strong>$22,532.81</strong> ($22.53K). This mean was taken in 1978. Next we’ll use random forest to explore the most important factors to housing prices.</p>
</section>
<section id="random-forest" class="level2">
<h2 class="anchored" data-anchor-id="random-forest">Random Forest</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>        IncNodePurity
rm         12574.0867
lstat      11898.8508
indus       3063.4200
nox         2747.0916
crim        2645.0800
ptratio     2607.5666
dis         2412.3478
tax         1387.2161
age         1105.2912
black        776.3449
rad          360.6068
chas         237.9042
zn           233.4437</code></pre>
</div>
</div>
<p>A decision tree is a model that resembles a flowchart that divides data into smaller groups according to feature values in order to make judgments. The objective is to optimize homogeneity, or purity, in each split, which means that the target variable’s values are comparable in each group. In <strong>Random Forest</strong>, <strong>IncNodePurity (Increase in Node Purity)</strong> is a measure of <strong>feature importance</strong>. It tells us <strong>how much a variable helps in reducing impurity (variance) in the decision trees</strong>.<br>
</p>
<p>There are <strong>nodes</strong> in every tree:<br>
<br>
Root Node 🌱 → Based on the most crucial feature, the initial decision split.<br>
Internal Nodes 🔗 → Decision points in between that further divide the data.<br>
Leaf Nodes 🍂→ The ultimate output, where predictions are produced</p>
<section id="how-it-works" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works">How it works:</h4>
<ul>
<li><p>Each decision tree in the forest <strong>splits the data at nodes</strong> based on different features.</p></li>
<li><p>The purity of each node is measured using <strong>Mean Squared Error (MSE) for regression</strong>. (If splitting on a variable reduces <strong>MSE by a large amount</strong>, it’s <strong>a good split</strong>)</p></li>
<li><p><strong>Higher IncNodePurity</strong> means the feature is more useful in <strong>splitting data and reducing error</strong>.</p></li>
</ul>
<p>According to a random forest model’s IncNodePurity measure, the most significant predictors for medv are <strong>lstat</strong> (poverty rate) and <strong>rm</strong> (number of rooms), which validate the correlation findings.<br>
Additional important indicators include <strong>ptratio</strong> (pupil-teacher ratio), <strong>nox</strong> (pollution), and <strong>dis</strong> (distance to job centers), suggesting that environmental and educational factors are important determinants of home values. <strong>Zn</strong> (the percentage of residential property allocated for big lots) and <strong>chas</strong> (closeness to the Charles River) are less significant factors.</p>
</section>
</section>
<section id="stepwise-regression" class="level2">
<h2 class="anchored" data-anchor-id="stepwise-regression">Stepwise Regression</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>Start:  AIC=1589.64
medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + 
    tax + ptratio + black + lstat

          Df Sum of Sq   RSS    AIC
- age      1      0.06 11079 1587.7
- indus    1      2.52 11081 1587.8
&lt;none&gt;                 11079 1589.6
- chas     1    218.97 11298 1597.5
- tax      1    242.26 11321 1598.6
- crim     1    243.22 11322 1598.6
- zn       1    257.49 11336 1599.3
- black    1    270.63 11349 1599.8
- rad      1    479.15 11558 1609.1
- nox      1    487.16 11566 1609.4
- ptratio  1   1194.23 12273 1639.4
- dis      1   1232.41 12311 1641.0
- rm       1   1871.32 12950 1666.6
- lstat    1   2410.84 13490 1687.3

Step:  AIC=1587.65
medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + 
    ptratio + black + lstat

          Df Sum of Sq   RSS    AIC
- indus    1      2.52 11081 1585.8
&lt;none&gt;                 11079 1587.7
+ age      1      0.06 11079 1589.6
- chas     1    219.91 11299 1595.6
- tax      1    242.24 11321 1596.6
- crim     1    243.20 11322 1596.6
- zn       1    260.32 11339 1597.4
- black    1    272.26 11351 1597.9
- rad      1    481.09 11560 1607.2
- nox      1    520.87 11600 1608.9
- ptratio  1   1200.23 12279 1637.7
- dis      1   1352.26 12431 1643.9
- rm       1   1959.55 13038 1668.0
- lstat    1   2718.88 13798 1696.7

Step:  AIC=1585.76
medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + 
    black + lstat

          Df Sum of Sq   RSS    AIC
&lt;none&gt;                 11081 1585.8
+ indus    1      2.52 11079 1587.7
+ age      1      0.06 11081 1587.8
- chas     1    227.21 11309 1594.0
- crim     1    245.37 11327 1594.8
- zn       1    257.82 11339 1595.4
- black    1    270.82 11352 1596.0
- tax      1    273.62 11355 1596.1
- rad      1    500.92 11582 1606.1
- nox      1    541.91 11623 1607.9
- ptratio  1   1206.45 12288 1636.0
- dis      1   1448.94 12530 1645.9
- rm       1   1963.66 13045 1666.3
- lstat    1   2723.48 13805 1695.0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + 
    tax + ptratio + black + lstat, data = Boston)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.5984  -2.7386  -0.5046   1.7273  26.2373 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***
crim         -0.108413   0.032779  -3.307 0.001010 ** 
zn            0.045845   0.013523   3.390 0.000754 ***
chas          2.718716   0.854240   3.183 0.001551 ** 
nox         -17.376023   3.535243  -4.915 1.21e-06 ***
rm            3.801579   0.406316   9.356  &lt; 2e-16 ***
dis          -1.492711   0.185731  -8.037 6.84e-15 ***
rad           0.299608   0.063402   4.726 3.00e-06 ***
tax          -0.011778   0.003372  -3.493 0.000521 ***
ptratio      -0.946525   0.129066  -7.334 9.24e-13 ***
black         0.009291   0.002674   3.475 0.000557 ***
lstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.736 on 494 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 
F-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<section id="analysis" class="level4">
<h4 class="anchored" data-anchor-id="analysis">Analysis</h4>
<p>The most optimal predictors in a regression model can be found using the feature selection method known as <strong>stepwise regression.</strong> By automating the variable selection procedure, it guarantees that the model only contains the most significant predictors.<br>
<br>
<strong>Statistical significance (p-values) and model performance (AIC/BIC/R2)</strong> determine whether variables are added or removed in stepwise regression.<br>
<br>
<strong>Different Stepwise Regression Types:</strong><br>
<strong>Forward Selection</strong>: If variables enhance the model, add them one at a time after starting with none.<br>
<strong>Backward Elimination:</strong> Begin by removing each variable in turn, starting with the least important.<br>
<strong>Stepwise selection:</strong> is a combination of the two methods; variables are included if they are helpful and removed if they later prove to be irrelevant.<br>
<br>
In this instance, we utilized <strong>Stepwise Selection</strong> as it’s most flexible and can prevent overfitting.</p>
</section>
<section id="process" class="level4">
<h4 class="anchored" data-anchor-id="process">Process:</h4>
<p><strong>Step 1:</strong> Start with All Variables</p>
<p>The full model includes all <strong>13 predictors</strong>. The A<strong>kaike Information Criterion (AIC)</strong> is <strong>1589.64,</strong> which measures model quality (lower = better).</p>
<p><strong>medv</strong>=β0​+β1​crim+β2​zn+β3​indus+β4​chas+β5​nox+β6​rm+β7​age+β8​dis+β9​rad+β10​tax+β11​ptratio+β12​black+β13​lstat+ϵ</p>
<p><strong>Step 2:</strong> Remove Least Important Variable</p>
<p>The step-by-step procedure evaluates the <strong>contribution</strong> of each variable. <strong>Removing</strong> a variable is done if it has no discernible effect on the accuracy of the model.</p>
<p>The <strong>first</strong> to be <strong>eliminated</strong> was <strong>age</strong> (old houses) due to the following: P-value (<strong>not statistically significant</strong>) = <strong>0.958.</strong> Didn’t improve <strong>R^2</strong> much better. <strong>AIC decreased</strong>, indicating a better model fit, to <strong>1587.7.</strong></p>
<p><strong>Step 3:</strong> Remove Other Unimportant Variables</p>
<p><strong>indus</strong> (non-retail land proportion) was removed because: <strong>p-value = 0.738</strong> (too high). <strong>AIC</strong> improved to <strong>1585.8,</strong> meaning the model became simpler without losing accuracy.</p>
<p>At this point, we are left with the <strong>11</strong> strongest variables with <strong>indus</strong> and <strong>age</strong> removed. (There can be other weak predictors but they were kept as they improve the model slightly).</p>
<p>🔹 <strong>Final Adjusted R² = 0.7348</strong><br>
🔹 <strong>Final AIC = 1585.8</strong></p>
</section>
</section>
<section id="fitting-the-regression-models" class="level2">
<h2 class="anchored" data-anchor-id="fitting-the-regression-models">fitting the regression models</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ ., data = boston_clean)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ lstat + rm, data = boston_clean)

Residuals:
    Min      1Q  Median      3Q     Max 
-18.076  -3.516  -1.010   1.909  28.131 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.35827    3.17283  -0.428    0.669    
lstat       -0.64236    0.04373 -14.689   &lt;2e-16 ***
rm           5.09479    0.44447  11.463   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.54 on 503 degrees of freedom
Multiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 
F-statistic: 444.3 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + 
    tax + ptratio + black + lstat, data = boston_clean)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.5984  -2.7386  -0.5046   1.7273  26.2373 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***
crim         -0.108413   0.032779  -3.307 0.001010 ** 
zn            0.045845   0.013523   3.390 0.000754 ***
chas          2.718716   0.854240   3.183 0.001551 ** 
nox         -17.376023   3.535243  -4.915 1.21e-06 ***
rm            3.801579   0.406316   9.356  &lt; 2e-16 ***
dis          -1.492711   0.185731  -8.037 6.84e-15 ***
rad           0.299608   0.063402   4.726 3.00e-06 ***
tax          -0.011778   0.003372  -3.493 0.000521 ***
ptratio      -0.946525   0.129066  -7.334 9.24e-13 ***
black         0.009291   0.002674   3.475 0.000557 ***
lstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.736 on 494 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 
F-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<section id="interpretation" class="level4">
<h4 class="anchored" data-anchor-id="interpretation">Interpretation</h4>
<p><strong>Model 1</strong> (all predictors):</p>
<ul>
<li><p><strong>Adjusted R² = 0.7338</strong> → Explains <strong>73.38% of the variance</strong> in home prices.</p></li>
<li><p><strong>Residual Standard Error (RSE) = 4.745</strong> → The average error in home price predictions is <strong>$4,745</strong>.</p></li>
<li><p><strong>F-statistic = 108.1, p &lt; 2.2e-16</strong> → The model is statistically significant, but includes some statistically insignificant predictors (which model 3 removes)</p></li>
</ul>
<p><strong>Model 2:</strong> (select 2 predictors)</p>
<ul>
<li><p><strong>Adjusted R² = 0.6371</strong> → Explains <strong>63.71% of the variance</strong> in home prices.</p></li>
<li><p><strong>Residual Standard Error = 5.54</strong> → Higher error than Model 1.</p></li>
<li><p><strong>F-statistic = 444.3, p &lt; 2.2e-16</strong> → The model is significant.</p></li>
</ul>
<p><strong>Model 3:</strong> (automatically selected 11 predictors)</p>
<ul>
<li><p><strong>Adjusted R² = 0.7348</strong> → Slightly better than Model 1.</p></li>
<li><p><strong>Residual Standard Error = 4.736</strong> → Lower than Model 1 (better predictive accuracy).</p></li>
<li><p><strong>F-statistic = 128.2, p &lt; 2.2e-16</strong> → Strong significance</p></li>
</ul>
<p>Model 3 so far has the highest explained variance and lowest error.</p>
</section>
<section id="why-we-cant-conclude-model-3-is-best-based-on-the-given-ouput" class="level4">
<h4 class="anchored" data-anchor-id="why-we-cant-conclude-model-3-is-best-based-on-the-given-ouput"><strong>Why we can’t conclude model 3 is best based on the given ouput:</strong></h4>
<ul>
<li><p>Standard Error (SE) and R2 (coefficient of determination) are helpful metrics for assessing how well a model fits <strong>training</strong> data, but they <strong>don’t</strong> reveal how well the model will work with <strong>new, unseen data</strong>, <strong>which is what’s important for creating predictions.</strong> (For example, a highly complex model can have an R^2 have 90% and a low standard error but it <strong>fails</strong> to predict accurately because the model <strong>overfits</strong> and captures noise instead of true relationships.</p></li>
<li><p>For this reason, before deciding on the optimal model, we require <strong>KNN</strong> or similar validation technique to test how well the model predicts unseen data.</p></li>
</ul>
</section>
</section>
<section id="knn-model-1" class="level2">
<h2 class="anchored" data-anchor-id="knn-model-1">KNN model 1</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>k-Nearest Neighbors 

306 samples
 13 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 275, 275, 275, 276, 275, 275, ... 
Resampling results across tuning parameters:

  k   RMSE      Rsquared   MAE     
   1  6.985692  0.5060300  4.970925
   2  6.257435  0.5566765  4.377462
   3  6.430624  0.5185585  4.417509
   4  6.223864  0.5479263  4.356895
   5  6.333470  0.5225443  4.444381
   6  6.377989  0.5101954  4.475222
   7  6.448804  0.4947965  4.617307
   8  6.481047  0.4897292  4.609556
   9  6.529151  0.4812203  4.628434
  10  6.637433  0.4644016  4.714632

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 4.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Root Mean Squared Error (RMSE): 6.646287 </code></pre>
</div>
</div>
</section>
<section id="knn-model-2" class="level2">
<h2 class="anchored" data-anchor-id="knn-model-2">KNN model 2</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>k-Nearest Neighbors 

306 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 275, 275, 275, 276, 275, 275, ... 
Resampling results across tuning parameters:

  k   RMSE      Rsquared   MAE     
   1  5.126110  0.7014945  3.646426
   2  4.545061  0.7457584  3.310407
   3  4.445402  0.7479194  3.146399
   4  4.344117  0.7620179  3.116306
   5  4.226525  0.7706582  3.018815
   6  4.184657  0.7728534  2.972095
   7  4.175576  0.7729026  2.995383
   8  4.190023  0.7709909  3.029928
   9  4.203446  0.7703049  3.037958
  10  4.251191  0.7676662  3.044116

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Root Mean Squared Error (RMSE): 4.981061 </code></pre>
</div>
</div>
</section>
<section id="knn-model-3" class="level2">
<h2 class="anchored" data-anchor-id="knn-model-3">KNN model 3</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>k-Nearest Neighbors 

306 samples
 11 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 275, 277, 276, 275, 276, 276, ... 
Resampling results across tuning parameters:

  k   RMSE      Rsquared   MAE     
   1  6.122807  0.5700169  4.267206
   2  5.969561  0.5522745  4.140640
   3  6.265553  0.5036003  4.302384
   4  6.287100  0.5052000  4.362933
   5  6.054120  0.5334745  4.149887
   6  6.335535  0.4856960  4.328062
   7  6.489349  0.4612534  4.466854
   8  6.643476  0.4390953  4.597001
   9  6.682019  0.4358769  4.590271
  10  6.714576  0.4353189  4.622883

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 2.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Root Mean Squared Error (RMSE): 6.202388 </code></pre>
</div>
</div>
<section id="information-on-knn-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="information-on-knn-algorithm">Information on KNN Algorithm:</h4>
<p>KNN is an instance-based, non-parametric learning algorithm that may be applied to both regression and classification problems. We use KNN because it assumes a non linear relationship in data (regression assumes linear which may not always be the case).<br>
<br>
<strong>KNN in Regression:</strong></p>
<ol type="1">
<li>Train the model using <strong>cross-validation</strong> (10-fold in this case) to prevent overfitting and to estimate the model’s <strong>performance</strong> more <strong>reliably</strong>.</li>
<li>Tune the k hyperparameter, <strong>(This is the number of neighbors used in the KNN algorithm to predict the output).</strong> This is done by testing multiple values of k (from 1 to 10) and selecting the optimal value based on the lowest <strong>Root Mean Squared Error (RMSE).</strong></li>
<li>The train() function trains a k-Nearest Neighbors regression model to <strong>predict</strong> the medv (Median Value of Owner-Occupied Homes) using all other variables as predictors.</li>
<li>It evaluates the <strong>performance</strong> of the model for each k value from the grid myGrid (e.g., testing k values between 1 and 10).</li>
<li>It uses 10-fold cross-validation to estimate the <strong>model’s accuracy</strong> and helps identify the best k value by evaluating the <strong>model’s performance</strong> (e.g., RMSE) for each fold and each k.</li>
</ol>
<p><strong>How K works:</strong><br>
To create a prediction, the algorithm considers the k nearest data points, called neighbors. It then determines the separation between two points. The most widely used approach is Euclidean Distance: The points are increasingly identical the closer they are to one another. The mean of the k-nearest neighbors serves as the forecast in KNN regression: The model finds the k closest houses and predicts median home value (<code>medv</code>) as the average price of those neighbors.</p>
</section>
<section id="fold-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="fold-cross-validation"><strong>10-Fold Cross-Validation:</strong></h3>
<p>1️⃣ <strong>The dataset is randomly split into 10 equal-sized folds.</strong> (If we have 306 samples each fold contains approx 30 samples)<br>
2️⃣ <strong>The model is trained on 9 folds</strong> and <strong>tested on the remaining 1 fold</strong>.<br>
3️⃣ <strong>This process repeats 10 times</strong>, each time using a different fold for testing.<br>
4️⃣ <strong>The final performance metric (e.g., RMSE, accuracy) is the average of all 10 test runs</strong>.</p>
<p><strong>Why do we need cross-validation in KNN?</strong></p>
<ul>
<li><strong>Helps find the best <code>k</code> for KNN</strong> (avoids overfitting/underfitting).</li>
</ul>
<section id="interpretation-of-results" class="level4">
<h4 class="anchored" data-anchor-id="interpretation-of-results">Interpretation of Results</h4>
<p>(results will be slightly different each time code is ran due to randomization of cross validation)</p>
<p>Model 1 (all predictors):</p>
<p>🔹 <strong>Final k chosen: <code>k=2</code> (lowest RMSE of 6.23)</strong><br>
🔹 <strong>R² = 0.53</strong>, meaning the model explains <strong>53% of variance</strong> in house prices.<br>
🔹 <strong>RMSE = 7.30</strong>, meaning the average price prediction error is <strong>$7,300</strong>.</p>
<p>Model 2 (chosen model)<br>
🔹 <strong>Final chosen <code>k=6</code> (smallest RMSE of 4.43)</strong><br>
🔹 <strong>R² = 0.74</strong>, meaning this model explains <strong>74% of the variance</strong> in home prices.<br>
🔹 <strong>RMSE = 4.43</strong>, meaning an average price prediction error of <strong>$4,430</strong>.</p>
<p>Model 3 (automated model w 11 predictors)</p>
<p>🔹 <strong>Final chosen <code>k=2</code> (smallest RMSE of 6.09)</strong><br>
🔹 <strong>R² = 0.56</strong>, meaning this model explains <strong>56% of price variance</strong>.<br>
🔹 <strong>RMSE = 6.41</strong>, meaning an average price prediction error of <strong>$6,410</strong>.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p><strong>The best overall model is KNN model 2 with <code>rm (# of rooms)</code> + <code>lstat</code> (population of lower status %)(RMSE = 4.43, R² = 0.74). Because the results are different from the linear regression of rm + lstat, this suggests that the relationship between home prices (<code>medv</code>) and predictors is slightly non-linear.</strong></p>
<p><strong>74% of variance explained is strong</strong>. <strong>RMSE of 4.43 is reasonable—errors are in the range of $4,430 on average</strong></p>
<p><strong>Though model 3 (11 predictors) had a higher R^2 and lower se when fitting the model a model that fits the training data too closely (over fitting) often ends up performing worse on new, unseen data.</strong></p>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>